{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6316527",
   "metadata": {},
   "source": [
    "imports / global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e35eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy\n",
    "import dlib\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "PREDICTOR_PATH = \"assets\\\\shape_predictor_68_face_landmarks.dat\"\n",
    "fedora = cv2.imread('images/cowboy.png', -1)\n",
    "round_glasses = cv2.imread('images/glasses.png', -1)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b77ef58",
   "metadata": {},
   "source": [
    "functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a749eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = dlib.shape_predictor(PREDICTOR_PATH)\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "def get_landmarks(im):\n",
    "    rects = detector(im, 1)\n",
    "\n",
    "    if len(rects) > 1:\n",
    "        return None\n",
    "    if len(rects) == 0:\n",
    "        return None\n",
    "\n",
    "    return numpy.matrix([[p.x, p.y] for p in predictor(im, rects[0]).parts()])\n",
    "\n",
    "def annotate_landmarks(im, landmarks):\n",
    "    im = im.copy()\n",
    "    for idx, point in enumerate(landmarks):\n",
    "        pos = (point[0, 0], point[0, 1])\n",
    "        cv2.putText(im, str(idx), pos,\n",
    "                    fontFace=cv2.FONT_HERSHEY_SCRIPT_SIMPLEX,\n",
    "                    fontScale=0.4,\n",
    "                    \n",
    "                    color=(0, 0, 255))\n",
    "        cv2.circle(im, pos, 3, color=(0, 255, 255))\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa37854",
   "metadata": {},
   "source": [
    "imported code for 'glasses' and 'hat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebcd9a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_eyes(glasses, landmarks, image_with_landmarks):\n",
    "    if landmarks is None:  \n",
    "        return image_with_landmarks\n",
    "\n",
    "    # find \"borders\"\n",
    "    glasses_width = landmarks[45][0, 0] - landmarks[36][0, 0]\n",
    "    glasses_height = landmarks[41][0, 1] - landmarks[37][0, 1]\n",
    "\n",
    "    # scale up\n",
    "    new_wid = int(glasses_width * 1.8)\n",
    "    new_hig = int(glasses_height * 4)\n",
    "    glasses = cv2.resize(glasses, (new_wid, new_hig))  \n",
    "\n",
    "    # find origin point (UL corner)\n",
    "    x_origin = int(landmarks[36][0, 0] - (new_wid - glasses_width) / 2)\n",
    "    y_origin = int(landmarks[36][0, 1] - new_hig / 2)\n",
    "\n",
    "    # find end point (LR corner) by adding width and height\n",
    "    x_end = x_origin + new_wid\n",
    "    y_end = y_origin + new_hig\n",
    "\n",
    "    # alpha channel shenanigans\n",
    "    alpha_glasses = glasses[:,:,3] / 255.0\n",
    "    alpha_large = 1.0 - alpha_glasses\n",
    "\n",
    "    # add together\n",
    "    for channel in range(0, 3):\n",
    "        image_with_landmarks[y_origin:y_end, x_origin:x_end, channel] = (\n",
    "            alpha_glasses * glasses[:, :, channel] + \n",
    "            alpha_large * image_with_landmarks[y_origin:y_end, x_origin:x_end, channel])\n",
    "    return image_with_landmarks\n",
    "    \n",
    "def add_hat(hat, landmarks, image_with_landmarks):\n",
    "    if landmarks is None:  \n",
    "        return image_with_landmarks\n",
    "\n",
    "    # find \"borders\"\n",
    "    hat_width = landmarks[26][0, 0] - landmarks[17][0, 0]\n",
    "    hat_height = int(hat.shape[1] * (hat_width / hat.shape[0]))\n",
    "\n",
    "    # scale up\n",
    "    new_wid = int(hat_width * 1.2)\n",
    "    new_hig = int(hat_height * 0.4)\n",
    "    hat = cv2.resize(hat, (new_wid, new_hig))  \n",
    "\n",
    "    # find \"original\" x & y origin\n",
    "    x_origin = int(landmarks[17][0, 0])\n",
    "    y_origin = int(landmarks[17][0, 1] - new_hig * 1.4)\n",
    "    # scuffed way to prevent errors from OOB\n",
    "\n",
    "    # left cut off\n",
    "    if x_origin < 0:\n",
    "        # cut off left to fit\n",
    "        hat = hat[0:hat.shape[0], -x_origin:hat.shape[1]]\n",
    "        # reduce width\n",
    "        new_wid = hat.shape[1]\n",
    "        # cap origin at 0\n",
    "        x_origin = 0\n",
    "\n",
    "    # right cut off\n",
    "    if x_origin + new_wid > image_with_landmarks.shape[1]:\n",
    "        # cut off right to fit\n",
    "        cutoff = x_origin + new_wid - image_with_landmarks.shape[1]\n",
    "        hat = hat[:, :hat.shape[1] - cutoff]\n",
    "        # reduce width - reduce by excess: end of image - start of x + witdth\n",
    "        new_wid = hat.shape[1]\n",
    "\n",
    "\n",
    "    if y_origin < 0:\n",
    "        # cut off top to fit\n",
    "        hat = hat[-y_origin:hat.shape[0], 0:hat.shape[1]]\n",
    "        # reduce hieght  \n",
    "        new_hig = hat.shape[0]\n",
    "        # cap origin at 0\n",
    "        y_origin = 0\n",
    "\n",
    "    # find end point (LR corner) by adding width and height\n",
    "    x_end = x_origin + new_wid\n",
    "    y_end = y_origin + new_hig\n",
    "\n",
    "    hat = hat[0:(y_end - y_origin), 0:(x_end - x_origin)]\n",
    "        \n",
    "    # alpha channel shenanigans\n",
    "    alpha_hat = hat[:,:,3] / 255.0\n",
    "    alpha_large = 1.0 - alpha_hat\n",
    "\n",
    "    # add together\n",
    "    for channel in range(0, 3):\n",
    "        image_with_landmarks[y_origin:y_end, x_origin:x_end, channel] = (\n",
    "            alpha_hat * hat[:, :, channel] + \n",
    "            alpha_large * image_with_landmarks[y_origin:y_end, x_origin:x_end, channel])\n",
    "        \n",
    "    return image_with_landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b59ed9-ab36-4310-bf04-ddbab4092a01",
   "metadata": {},
   "source": [
    "mediafire code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d9eb910-1905-42c0-ac0f-a347b221cc03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# taken from mediafire example code.\n",
    "BG_COLOR = (192, 192, 192) # gray\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_selfie_segmentation.SelfieSegmentation(\n",
    "    model_selection=1) as selfie_segmentation:\n",
    "    bg_image = cv2.imread('images/desert.jpg')\n",
    "    camera_size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), \n",
    "                   int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    bg_image = cv2.resize(bg_image, camera_size, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        \n",
    "        if not success:\n",
    "          print(\"Ignoring empty camera frame.\")\n",
    "          # If loading a video, use 'break' instead of 'continue'.\n",
    "          continue\n",
    "    \n",
    "        # Flip the image horizontally for a later selfie-view display, and convert\n",
    "        # the BGR image to RGB.\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        cv2.imshow(\"Flipped Original\", image)\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        results = selfie_segmentation.process(image)\n",
    "    \n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "        # Draw selfie segmentation on the background image.\n",
    "        # To improve segmentation around boundaries, consider applying a joint\n",
    "        # bilateral filter to \"results.segmentation_mask\" with \"image\".\n",
    "        condition = np.stack(\n",
    "          (results.segmentation_mask,) * 3, axis=-1) > 0.1\n",
    "        # The background can be customized.\n",
    "        #   a) Load an image (with the same width and height of the input image) to\n",
    "        #      be the background, e.g., bg_image = cv2.imread('/path/to/image/file')\n",
    "        #   b) Blur the input image by applying image filtering, e.g.,\n",
    "        #      bg_image = cv2.GaussianBlur(image,(55,55),0)\n",
    "        mask = results.segmentation_mask\n",
    "        blurred_mask = cv2.GaussianBlur(mask, (15, 15), 0)\n",
    "        alpha = np.expand_dims(blurred_mask, axis=-1)\n",
    "        cv2.imshow(\"Mask\", alpha)\n",
    "        \n",
    "        if bg_image is None:\n",
    "          bg_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "          bg_image[:] = BG_COLOR\n",
    "            \n",
    "        output_image = (alpha * image + (1 - alpha) * bg_image).astype(np.uint8)\n",
    "        cv2.imshow(\"Applied Background\", output_image)\n",
    "        landmarks = get_landmarks(output_image)\n",
    "        cv2.imshow(\"Landmarks\", annotate_landmarks(output_image, landmarks))\n",
    "        \n",
    "        \n",
    "        if landmarks is not None:\n",
    "            output_image = add_hat(fedora, landmarks, output_image)\n",
    "            output_image = add_to_eyes(round_glasses, landmarks, output_image)\n",
    "    \n",
    "        cv2.imshow('Output', output_image)\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "          break\n",
    "            \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d0575",
   "metadata": {},
   "source": [
    "UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7734575a-8a68-44fc-87cb-159244180010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "from PySide6.QtCore import Qt, QThread, Signal, Slot\n",
    "from PySide6.QtGui import QAction, QImage, QKeySequence, QPixmap\n",
    "from PySide6.QtWidgets import (QApplication, QComboBox, QGroupBox,\n",
    "                               QHBoxLayout, QLabel, QMainWindow, QPushButton,\n",
    "                               QSizePolicy, QVBoxLayout, QWidget)\n",
    "class Window(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Title and dimensions\n",
    "        self.setWindowTitle(\"Photo Booth\")\n",
    "        self.setGeometry(0, 0, 800, 500)\n",
    "\n",
    "        # Main menu bar\n",
    "        self.menu = self.menuBar()\n",
    "        self.menu_file = self.menu.addMenu(\"File\")\n",
    "        exit = QAction(\"Exit\", self, triggered=qApp.quit)  # noqa: F821\n",
    "        self.menu_file.addAction(exit)\n",
    "\n",
    "        # Create a label for the display camera\n",
    "        self.label = QLabel(self)\n",
    "        self.label.setFixedSize(640, 480)\n",
    "\n",
    "        # Thread in charge of updating the image\n",
    "        self.th = Thread(self)\n",
    "        self.th.finished.connect(self.close)\n",
    "        self.th.updateFrame.connect(self.setImage)\n",
    "\n",
    "        # Model group\n",
    "        self.group_model = QGroupBox(\"Apply Settings:\")\n",
    "        self.group_model.setSizePolicy(QSizePolicy.Policy.Preferred, QSizePolicy.Policy.Expanding)\n",
    "        model_layout = QVBoxLayout()\n",
    "\n",
    "        # background\n",
    "        self.bg_dropdown = QComboBox()\n",
    "        self.bg_dropdown.addItems([\"None\", \"Space\", \"Desert\"])\n",
    "        model_layout.addWidget(QLabel(\"Background:\"))\n",
    "        model_layout.addWidget(self.bg_dropdown)\n",
    "        self.bg_dropdown.currentTextChanged.connect(self.bg_changed)\n",
    "\n",
    "        # hat\n",
    "        self.hat_dropdown = QComboBox()\n",
    "        self.hat_dropdown.addItems([\"None\", \"Fedora\", \"Cowboy\"])\n",
    "        model_layout.addWidget(QLabel(\"Hat:\"))\n",
    "        model_layout.addWidget(self.hat_dropdown)\n",
    "        self.hat_dropdown.currentTextChanged.connect(self.hat_changed)\n",
    "\n",
    "        # glasses\n",
    "        self.glasses_dropdown = QComboBox()\n",
    "        self.glasses_dropdown.addItems([\"None\", \"Square-ish\", \"Round\"])\n",
    "        model_layout.addWidget(QLabel(\"Glasses:\"))\n",
    "        model_layout.addWidget(self.glasses_dropdown)\n",
    "        self.glasses_dropdown.currentTextChanged.connect(self.glasses_changed)\n",
    "        \n",
    "        self.group_model.setLayout(model_layout)\n",
    "\n",
    "        # Buttons layout\n",
    "        buttons_layout = QHBoxLayout()\n",
    "        self.button1 = QPushButton(\"Start\")\n",
    "        self.button2 = QPushButton(\"Stop/Close\")\n",
    "        self.button1.setSizePolicy(QSizePolicy.Policy.Preferred, QSizePolicy.Policy.Expanding)\n",
    "        self.button2.setSizePolicy(QSizePolicy.Policy.Preferred, QSizePolicy.Policy.Expanding)\n",
    "        buttons_layout.addWidget(self.button2)\n",
    "        buttons_layout.addWidget(self.button1)\n",
    "\n",
    "        right_layout = QHBoxLayout()\n",
    "        right_layout.addWidget(self.group_model, 1)\n",
    "        right_layout.addLayout(buttons_layout, 1)\n",
    "\n",
    "        # Main layout\n",
    "        layout = QVBoxLayout()\n",
    "        layout.addWidget(self.label)\n",
    "        layout.addLayout(right_layout)\n",
    "\n",
    "        # Central widget\n",
    "        widget = QWidget(self)\n",
    "        widget.setLayout(layout)\n",
    "        self.setCentralWidget(widget)\n",
    "\n",
    "        # Connections\n",
    "        self.button1.clicked.connect(self.start)\n",
    "        self.button2.clicked.connect(self.kill_thread)\n",
    "        self.button2.setEnabled(False)\n",
    "\n",
    "    @Slot()\n",
    "    def hat_changed(self, new):\n",
    "        self.th.set_hat(new)\n",
    "\n",
    "    @Slot()\n",
    "    def bg_changed(self, new):\n",
    "        self.th.set_bg_image(new)\n",
    "\n",
    "    @Slot()\n",
    "    def glasses_changed(self, new):\n",
    "        self.th.set_glasses(new)\n",
    "    \n",
    "    @Slot()\n",
    "    def set_model(self, text):\n",
    "        self.th.set_file(text)\n",
    "\n",
    "    @Slot()\n",
    "    def kill_thread(self):\n",
    "        print(\"Finishing...\")\n",
    "        self.button2.setEnabled(False)\n",
    "        self.button1.setEnabled(True)\n",
    "        self.th.cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        self.status = False\n",
    "        self.th.terminate()\n",
    "        # Give time for the thread to finish\n",
    "        time.sleep(1)\n",
    "\n",
    "    @Slot()\n",
    "    def start(self):\n",
    "        print(\"Starting...\")\n",
    "        self.button2.setEnabled(True)\n",
    "        self.button1.setEnabled(False)\n",
    "        self.th.start()\n",
    "\n",
    "    @Slot(QImage)\n",
    "    def setImage(self, image):\n",
    "        self.label.setPixmap(QPixmap.fromImage(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f2cfdf5-220a-4291-81c5-d98b3bd58c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Thread(QThread):\n",
    "    updateFrame = Signal(QImage)\n",
    "\n",
    "    def __init__(self, parent=None):\n",
    "        QThread.__init__(self, parent)\n",
    "        self.trained_file = None\n",
    "        self.status = True\n",
    "        self.cap = True\n",
    "\n",
    "        self.current_hat = None\n",
    "        self.current_glasses = None\n",
    "        self.bg_image = None\n",
    "\n",
    "    def set_hat(self, new):\n",
    "        print(new)\n",
    "        if(new == \"None\"):\n",
    "            self.current_hat = None\n",
    "        elif(new == \"Fedora\"):\n",
    "            self.current_hat = cv2.imread(\"images/fedora.png\", cv2.IMREAD_UNCHANGED)\n",
    "        elif(new == \"Cowboy\"):\n",
    "            self.current_hat = cv2.imread(\"images/cowboy.png\", cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    def set_glasses(self, new):\n",
    "        print(new)\n",
    "        if(new == \"None\"):\n",
    "            self.current_glasses = None\n",
    "        elif(new == \"Square-ish\"):\n",
    "            self.current_glasses = cv2.imread(\"images/glasses.png\", cv2.IMREAD_UNCHANGED)\n",
    "        elif(new == \"Round\"):\n",
    "            self.current_glasses = cv2.imread(\"images/round_glasses.png\", cv2.IMREAD_UNCHANGED)\n",
    "    \n",
    "    def set_bg_image(self, new):\n",
    "        if(new == \"None\"):\n",
    "           self.bg_image = None\n",
    "        elif(new == \"Space\"):\n",
    "            self.bg_image = cv2.imread(\"images/space.jpg\", cv2.IMREAD_UNCHANGED)\n",
    "        elif(new == \"Desert\"):\n",
    "            self.bg_image = cv2.imread(\"images/desert.jpg\", cv2.IMREAD_UNCHANGED)\n",
    "        else:\n",
    "            self.bg_image = None\n",
    "        print(new)\n",
    "    \n",
    "    def run(self):\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as selfie_segmentation:\n",
    "            \n",
    "            while self.status:\n",
    "                ret, frame = self.cap.read()\n",
    "        \n",
    "                if not ret:\n",
    "                  #print(\"Ignoring empty camera frame.\")\n",
    "                  continue\n",
    "\n",
    "                # flip image to act as \"mirror\"\n",
    "                image = cv2.flip(frame, 1)\n",
    "\n",
    "                if self.bg_image is not None:\n",
    "                    bg_image = cv2.resize(self.bg_image, camera_size, interpolation = cv2.INTER_AREA)\n",
    "                    # convert into rgb to make model work\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    # To improve performance, optionally mark the image as not writeable to\n",
    "                    # pass by reference.\n",
    "                    image.flags.writeable = False\n",
    "\n",
    "                    # process image with model (?)\n",
    "                    results = selfie_segmentation.process(image)\n",
    "                \n",
    "                    image.flags.writeable = True\n",
    "\n",
    "                    # convert back into the weirdo format\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                    # Draw selfie segmentation on the background image.\n",
    "                    # To improve segmentation around boundaries, consider applying a joint\n",
    "                    # bilateral filter to \"results.segmentation_mask\" with \"image\".\n",
    "                    condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.1\n",
    "                    # The background can be customized.\n",
    "                    #   a) Load an image (with the same width and height of the input image) to\n",
    "                    #      be the background, e.g., bg_image = cv2.imread('/path/to/image/file')\n",
    "                    #   b) Blur the input image by applying image filtering, e.g.,\n",
    "                    #      bg_image = cv2.GaussianBlur(image,(55,55),0)\n",
    "                    mask = results.segmentation_mask\n",
    "\n",
    "                    # blur the mask a bit to smooth out edges\n",
    "                    blurred_mask = cv2.GaussianBlur(mask, (15, 15), 0)\n",
    "                    alpha = np.expand_dims(blurred_mask, axis=-1)\n",
    "\n",
    "                    # apply background using mask\n",
    "                    image = (alpha * image + (1 - alpha) * bg_image).astype(np.uint8)\n",
    "                \n",
    "                landmarks = get_landmarks(image)\n",
    "                #image = annotate_landmarks(image, landmarks)\n",
    "                \n",
    "                if landmarks is not None:\n",
    "                    if self.current_hat is not None:\n",
    "                        image = add_hat(self.current_hat, landmarks, image)\n",
    "                    if self.current_glasses is not None:\n",
    "                        image = add_to_eyes(self.current_glasses, landmarks, image)\n",
    "\n",
    "                # reprocess into rgb because cv2 is weird like that\n",
    "                output_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                h, w, ch = output_image.shape\n",
    "                img = QImage(output_image.data, w, h, ch * w, QImage.Format.Format_RGB888)\n",
    "                scaled_img = img.scaled(640, 480, Qt.AspectRatioMode.KeepAspectRatio)\n",
    "    \n",
    "                # Emit signal\n",
    "                self.updateFrame.emit(scaled_img)\n",
    "        sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb2edd35-a8b8-4b02-a941-f739663b568c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "Space\n",
      "Desert\n",
      "Fedora\n",
      "Cowboy\n",
      "Square-ish\n",
      "Round\n",
      "None\n",
      "Fedora\n",
      "Finishing...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app = QApplication.instance() or QApplication(sys.argv)\n",
    "    w = Window()\n",
    "    w.show()\n",
    "    sys.exit(app.exec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76b593-c828-409c-8363-b4ed66748bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
