{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6316527",
   "metadata": {},
   "source": [
    "imports / global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e35eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy\n",
    "import dlib\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "PREDICTOR_PATH = \"assets\\\\shape_predictor_68_face_landmarks.dat\"\n",
    "fedora = cv2.imread('images/fedora.png', -1)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b77ef58",
   "metadata": {},
   "source": [
    "functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a749eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = dlib.shape_predictor(PREDICTOR_PATH)\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "def get_landmarks(im):\n",
    "    rects = detector(im, 1)\n",
    "\n",
    "    if len(rects) > 1:\n",
    "        return None\n",
    "    if len(rects) == 0:\n",
    "        return None\n",
    "\n",
    "    return numpy.matrix([[p.x, p.y] for p in predictor(im, rects[0]).parts()])\n",
    "\n",
    "def annotate_landmarks(im, landmarks):\n",
    "    im = im.copy()\n",
    "    for idx, point in enumerate(landmarks):\n",
    "        pos = (point[0, 0], point[0, 1])\n",
    "        cv2.putText(im, str(idx), pos,\n",
    "                    fontFace=cv2.FONT_HERSHEY_SCRIPT_SIMPLEX,\n",
    "                    fontScale=0.4,\n",
    "                    \n",
    "                    color=(0, 0, 255))\n",
    "        cv2.circle(im, pos, 3, color=(0, 255, 255))\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa37854",
   "metadata": {},
   "source": [
    "imported code for 'glasses' and 'hat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebcd9a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_eyes(glasses, landmarks, image_with_landmarks):\n",
    "    if landmarks is None:  \n",
    "        return image_with_landmarks\n",
    "\n",
    "    # find \"borders\"\n",
    "    glasses_width = landmarks[45][0, 0] - landmarks[36][0, 0]\n",
    "    glasses_height = landmarks[41][0, 1] - landmarks[37][0, 1]\n",
    "\n",
    "    # scale up\n",
    "    new_wid = int(glasses_width * 1.8)\n",
    "    new_hig = int(glasses_height * 4)\n",
    "    glasses = cv2.resize(glasses, (new_wid, new_hig))  \n",
    "\n",
    "    # find origin point (UL corner)\n",
    "    x_origin = int(landmarks[36][0, 0] - (new_wid - glasses_width) / 2)\n",
    "    y_origin = int(landmarks[36][0, 1] - new_hig / 2)\n",
    "\n",
    "    # find end point (LR corner) by adding width and height\n",
    "    x_end = x_origin + new_wid\n",
    "    y_end = y_origin + new_hig\n",
    "\n",
    "    # alpha channel shenanigans\n",
    "    alpha_glasses = glasses[:,:,3] / 255.0\n",
    "    alpha_large = 1.0 - alpha_glasses\n",
    "\n",
    "    # add together\n",
    "    for channel in range(0, 3):\n",
    "        image_with_landmarks[y_origin:y_end, x_origin:x_end, channel] = (\n",
    "            alpha_glasses * glasses[:, :, channel] + \n",
    "            alpha_large * image_with_landmarks[y_origin:y_end, x_origin:x_end, channel])\n",
    "    return image_with_landmarks\n",
    "    \n",
    "def add_hat(hat, landmarks, image_with_landmarks):\n",
    "    if landmarks is None:  \n",
    "        return image_with_landmarks\n",
    "\n",
    "    # find \"borders\"\n",
    "    hat_width = landmarks[26][0, 0] - landmarks[17][0, 0]\n",
    "    hat_height = int(hat.shape[1] * (hat_width / hat.shape[0]))\n",
    "\n",
    "    # scale up\n",
    "    new_wid = int(hat_width * 1.2)\n",
    "    new_hig = int(hat_height * 0.4)\n",
    "    hat = cv2.resize(hat, (new_wid, new_hig))  \n",
    "\n",
    "    # find \"original\" x & y origin\n",
    "    x_origin = int(landmarks[17][0, 0])\n",
    "    y_origin = int(landmarks[17][0, 1] - new_hig * 1.4)\n",
    "    # scuffed way to prevent errors from OOB\n",
    "\n",
    "    # left cut off\n",
    "    if x_origin < 0:\n",
    "        # cut off left to fit\n",
    "        hat = hat[0:hat.shape[0], -x_origin:hat.shape[1]]\n",
    "        # reduce width\n",
    "        new_wid = hat.shape[1]\n",
    "        # cap origin at 0\n",
    "        x_origin = 0\n",
    "\n",
    "    # right cut off\n",
    "    if x_origin + new_wid > image_with_landmarks.shape[1]:\n",
    "        # cut off right to fit\n",
    "        cutoff = x_origin + new_wid - image_with_landmarks.shape[1]\n",
    "        hat = hat[:, :hat.shape[1] - cutoff]\n",
    "        # reduce width - reduce by excess: end of image - start of x + witdth\n",
    "        new_wid = hat.shape[1]\n",
    "\n",
    "\n",
    "    if y_origin < 0:\n",
    "        # cut off top to fit\n",
    "        hat = hat[-y_origin:hat.shape[0], 0:hat.shape[1]]\n",
    "        # reduce hieght  \n",
    "        new_hig = hat.shape[0]\n",
    "        # cap origin at 0\n",
    "        y_origin = 0\n",
    "\n",
    "    # find end point (LR corner) by adding width and height\n",
    "    x_end = x_origin + new_wid\n",
    "    y_end = y_origin + new_hig\n",
    "\n",
    "    hat = hat[0:(y_end - y_origin), 0:(x_end - x_origin)]\n",
    "        \n",
    "    # alpha channel shenanigans\n",
    "    alpha_hat = hat[:,:,3] / 255.0\n",
    "    alpha_large = 1.0 - alpha_hat\n",
    "\n",
    "    # add together\n",
    "    for channel in range(0, 3):\n",
    "        image_with_landmarks[y_origin:y_end, x_origin:x_end, channel] = (\n",
    "            alpha_hat * hat[:, :, channel] + \n",
    "            alpha_large * image_with_landmarks[y_origin:y_end, x_origin:x_end, channel])\n",
    "        \n",
    "    return image_with_landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b59ed9-ab36-4310-bf04-ddbab4092a01",
   "metadata": {},
   "source": [
    "mediafire code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d9eb910-1905-42c0-ac0f-a347b221cc03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# taken from mediafire example code.\n",
    "BG_COLOR = (192, 192, 192) # gray\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_selfie_segmentation.SelfieSegmentation(\n",
    "    model_selection=1) as selfie_segmentation:\n",
    "    bg_image = cv2.imread('assets/desert.jpg')\n",
    "    camera_size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), \n",
    "                   int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    bg_image = cv2.resize(bg_image, camera_size, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        \n",
    "        if not success:\n",
    "          print(\"Ignoring empty camera frame.\")\n",
    "          # If loading a video, use 'break' instead of 'continue'.\n",
    "          continue\n",
    "    \n",
    "        # Flip the image horizontally for a later selfie-view display, and convert\n",
    "        # the BGR image to RGB.\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        results = selfie_segmentation.process(image)\n",
    "    \n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "        # Draw selfie segmentation on the background image.\n",
    "        # To improve segmentation around boundaries, consider applying a joint\n",
    "        # bilateral filter to \"results.segmentation_mask\" with \"image\".\n",
    "        condition = np.stack(\n",
    "          (results.segmentation_mask,) * 3, axis=-1) > 0.1\n",
    "        # The background can be customized.\n",
    "        #   a) Load an image (with the same width and height of the input image) to\n",
    "        #      be the background, e.g., bg_image = cv2.imread('/path/to/image/file')\n",
    "        #   b) Blur the input image by applying image filtering, e.g.,\n",
    "        #      bg_image = cv2.GaussianBlur(image,(55,55),0)\n",
    "        mask = results.segmentation_mask\n",
    "        blurred_mask = cv2.GaussianBlur(mask, (15, 15), 0)\n",
    "        alpha = np.expand_dims(blurred_mask, axis=-1)\n",
    "        \n",
    "        if bg_image is None:\n",
    "          bg_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "          bg_image[:] = BG_COLOR\n",
    "            \n",
    "        output_image = (alpha * image + (1 - alpha) * bg_image).astype(np.uint8)\n",
    "        landmarks = get_landmarks(output_image)\n",
    "        \n",
    "        if landmarks is not None:\n",
    "            output_image = add_hat(fedora, landmarks, output_image)\n",
    "    \n",
    "        cv2.imshow('MediaPipe Selfie Segmentation', output_image)\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "          break\n",
    "            \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d0575",
   "metadata": {},
   "source": [
    "UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0ee9f71-4691-429a-8fef-b74d99f62cd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Window' object has no attribute 'combobox'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 209\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    208\u001b[39m     app = QApplication.instance() \u001b[38;5;129;01mor\u001b[39;00m QApplication(sys.argv)\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     w = \u001b[43mWindow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m     w.show()\n\u001b[32m    211\u001b[39m     sys.exit(app.exec())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 172\u001b[39m, in \u001b[36mWindow.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28mself\u001b[39m.button2.clicked.connect(\u001b[38;5;28mself\u001b[39m.kill_thread)\n\u001b[32m    171\u001b[39m \u001b[38;5;28mself\u001b[39m.button2.setEnabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcombobox\u001b[49m.currentTextChanged.connect(\u001b[38;5;28mself\u001b[39m.set_model)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Window' object has no attribute 'combobox'"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "from PySide6.QtCore import Qt, QThread, Signal, Slot\n",
    "from PySide6.QtGui import QAction, QImage, QKeySequence, QPixmap\n",
    "from PySide6.QtWidgets import (QApplication, QComboBox, QGroupBox,\n",
    "                               QHBoxLayout, QLabel, QMainWindow, QPushButton,\n",
    "                               QSizePolicy, QVBoxLayout, QWidget)\n",
    "\n",
    "\n",
    "\"\"\"This example uses the video from a  webcam to apply pattern\n",
    "detection from the OpenCV module. e.g.: face, eyes, body, etc.\"\"\"\n",
    "\n",
    "\n",
    "class Thread(QThread):\n",
    "    updateFrame = Signal(QImage)\n",
    "\n",
    "    def __init__(self, parent=None):\n",
    "        QThread.__init__(self, parent)\n",
    "        self.trained_file = None\n",
    "        self.status = True\n",
    "        self.cap = True\n",
    "\n",
    "        self.current_hat = None\n",
    "\n",
    "    def set_hat(self, new):\n",
    "        print(new)\n",
    "        if(new == \"None\"):\n",
    "            self.current_hat = None\n",
    "        elif(new == \"Fedora\"):\n",
    "            self.current_hat = cv2.imread(\"images/fedora.png\", cv2.IMREAD_UNCHANGED)\n",
    "        elif(new == \"Cowboy\"):\n",
    "            self.current_hat = cv2.imread(\"images/cowboy.png\", cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    \n",
    "    def run(self):\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        with mp_selfie_segmentation.SelfieSegmentation(\n",
    "        model_selection=1) as selfie_segmentation:\n",
    "            bg_image = cv2.imread('assets/desert.jpg')\n",
    "            camera_size = (int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH)), \n",
    "                           int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "            bg_image = cv2.resize(bg_image, camera_size, interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "            while self.status:\n",
    "                cascade = cv2.CascadeClassifier(self.trained_file)\n",
    "                ret, frame = self.cap.read()\n",
    "        \n",
    "                if not ret:\n",
    "                  print(\"Ignoring empty camera frame.\")\n",
    "                  # If loading a video, use 'break' instead of 'continue'.\n",
    "                  continue\n",
    "            \n",
    "                # Flip the image horizontally for a later selfie-view display, and convert\n",
    "                # the BGR image to RGB.\n",
    "                image = cv2.cvtColor(cv2.flip(frame, 1), cv2.COLOR_BGR2RGB)\n",
    "                # To improve performance, optionally mark the image as not writeable to\n",
    "                # pass by reference.\n",
    "                image.flags.writeable = False\n",
    "                results = selfie_segmentation.process(image)\n",
    "            \n",
    "                image.flags.writeable = True\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "                # Draw selfie segmentation on the background image.\n",
    "                # To improve segmentation around boundaries, consider applying a joint\n",
    "                # bilateral filter to \"results.segmentation_mask\" with \"image\".\n",
    "                condition = np.stack(\n",
    "                  (results.segmentation_mask,) * 3, axis=-1) > 0.1\n",
    "                # The background can be customized.\n",
    "                #   a) Load an image (with the same width and height of the input image) to\n",
    "                #      be the background, e.g., bg_image = cv2.imread('/path/to/image/file')\n",
    "                #   b) Blur the input image by applying image filtering, e.g.,\n",
    "                #      bg_image = cv2.GaussianBlur(image,(55,55),0)\n",
    "                mask = results.segmentation_mask\n",
    "                blurred_mask = cv2.GaussianBlur(mask, (15, 15), 0)\n",
    "                alpha = np.expand_dims(blurred_mask, axis=-1)\n",
    "                \n",
    "                if bg_image is None:\n",
    "                  bg_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "                  bg_image[:] = BG_COLOR\n",
    "                    \n",
    "                output_image = (alpha * image + (1 - alpha) * bg_image).astype(np.uint8)\n",
    "                landmarks = get_landmarks(output_image)\n",
    "                \n",
    "                if landmarks is not None:\n",
    "                    if self.current_hat is not None:\n",
    "                        output_image = add_hat(self.current_hat, landmarks, output_image)\n",
    "\n",
    "                h, w, ch = output_image.shape\n",
    "                img = QImage(output_image.data, w, h, ch * w, QImage.Format.Format_RGB888)\n",
    "                scaled_img = img.scaled(640, 480, Qt.AspectRatioMode.KeepAspectRatio)\n",
    "    \n",
    "                # Emit signal\n",
    "                self.updateFrame.emit(scaled_img)\n",
    "        sys.exit(-1)\n",
    "\n",
    "\n",
    "class Window(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Title and dimensions\n",
    "        self.setWindowTitle(\"Photo Booth\")\n",
    "        self.setGeometry(0, 0, 800, 500)\n",
    "\n",
    "        # Main menu bar\n",
    "        self.menu = self.menuBar()\n",
    "        self.menu_file = self.menu.addMenu(\"File\")\n",
    "        exit = QAction(\"Exit\", self, triggered=qApp.quit)  # noqa: F821\n",
    "        self.menu_file.addAction(exit)\n",
    "\n",
    "        self.menu_about = self.menu.addMenu(\"&About\")\n",
    "        about = QAction(\"About Qt\", self,\n",
    "                        shortcut=QKeySequence(QKeySequence.StandardKey.HelpContents),\n",
    "                        triggered=qApp.aboutQt)  # noqa: F821\n",
    "        self.menu_about.addAction(about)\n",
    "\n",
    "        # Create a label for the display camera\n",
    "        self.label = QLabel(self)\n",
    "        self.label.setFixedSize(640, 480)\n",
    "\n",
    "        # Thread in charge of updating the image\n",
    "        self.th = Thread(self)\n",
    "        self.th.finished.connect(self.close)\n",
    "        self.th.updateFrame.connect(self.setImage)\n",
    "\n",
    "        # Model group\n",
    "        self.group_model = QGroupBox(\"Trained model\")\n",
    "        self.group_model.setSizePolicy(QSizePolicy.Policy.Preferred, QSizePolicy.Policy.Expanding)\n",
    "        model_layout = QHBoxLayout()\n",
    "\n",
    "        self.hat_dropdown = QComboBox()\n",
    "        self.hat_dropdown.addItems([\"None\", \"Fedora\", \"Cowboy\"])\n",
    "        model_layout.addWidget(QLabel(\"Hat:\"), 10)\n",
    "        model_layout.addWidget(self.hat_dropdown, 90)\n",
    "\n",
    "        self.hat_dropdown.currentTextChanged.connect(self.hat_changed)\n",
    "        \n",
    "        self.group_model.setLayout(model_layout)\n",
    "\n",
    "        # Buttons layout\n",
    "        buttons_layout = QHBoxLayout()\n",
    "        self.button1 = QPushButton(\"Start\")\n",
    "        self.button2 = QPushButton(\"Stop/Close\")\n",
    "        self.button1.setSizePolicy(QSizePolicy.Policy.Preferred, QSizePolicy.Policy.Expanding)\n",
    "        self.button2.setSizePolicy(QSizePolicy.Policy.Preferred, QSizePolicy.Policy.Expanding)\n",
    "        buttons_layout.addWidget(self.button2)\n",
    "        buttons_layout.addWidget(self.button1)\n",
    "\n",
    "        right_layout = QHBoxLayout()\n",
    "        right_layout.addWidget(self.group_model, 1)\n",
    "        right_layout.addLayout(buttons_layout, 1)\n",
    "\n",
    "        # Main layout\n",
    "        layout = QVBoxLayout()\n",
    "        layout.addWidget(self.label)\n",
    "        layout.addLayout(right_layout)\n",
    "\n",
    "        # Central widget\n",
    "        widget = QWidget(self)\n",
    "        widget.setLayout(layout)\n",
    "        self.setCentralWidget(widget)\n",
    "\n",
    "        # Connections\n",
    "        self.button1.clicked.connect(self.start)\n",
    "        self.button2.clicked.connect(self.kill_thread)\n",
    "        self.button2.setEnabled(False)\n",
    "\n",
    "    @Slot()\n",
    "    def hat_changed(self, new):\n",
    "        self.th.set_hat(new)\n",
    "    \n",
    "    @Slot()\n",
    "    def set_model(self, text):\n",
    "        self.th.set_file(text)\n",
    "\n",
    "    @Slot()\n",
    "    def kill_thread(self):\n",
    "        print(\"Finishing...\")\n",
    "        self.button2.setEnabled(False)\n",
    "        self.button1.setEnabled(True)\n",
    "        self.th.cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        self.status = False\n",
    "        self.th.terminate()\n",
    "        # Give time for the thread to finish\n",
    "        time.sleep(1)\n",
    "\n",
    "    @Slot()\n",
    "    def start(self):\n",
    "        print(\"Starting...\")\n",
    "        self.button2.setEnabled(True)\n",
    "        self.button1.setEnabled(False)\n",
    "        self.th.set_file(self.combobox.currentText())\n",
    "        self.th.start()\n",
    "\n",
    "    @Slot(QImage)\n",
    "    def setImage(self, image):\n",
    "        self.label.setPixmap(QPixmap.fromImage(image))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = QApplication.instance() or QApplication(sys.argv)\n",
    "    w = Window()\n",
    "    w.show()\n",
    "    sys.exit(app.exec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76b593-c828-409c-8363-b4ed66748bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (with OpenCV)",
   "language": "python",
   "name": "py312cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
